{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indra622/AIAcademy_SpeechRecognition/blob/main/nemo_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nemo Installation"
      ],
      "metadata": {
        "id": "jsCEydQbFU72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF4n2g6PgXup"
      },
      "outputs": [],
      "source": [
        "!pip install nemo_toolkit['all']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained model \n",
        "\n",
        "Model: Conformer-large (https://www.isca-speech.org/archive/interspeech_2020/gulati20_interspeech.html)\n",
        "\n",
        "Dataset: LibriSpeech (https://openslr.org/12/)\n",
        "\n",
        "Tokenizer: Byte-pair Encoding by Sentencepiece (https://github.com/google/sentencepiece)\n",
        "\n"
      ],
      "metadata": {
        "id": "L1FHH6WqFcij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model load"
      ],
      "metadata": {
        "id": "tX-kIZVxhwu-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCp5mWtEgag0"
      },
      "outputs": [],
      "source": [
        "import nemo.collections.asr as nemo_asr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTybIfeYh7JI"
      },
      "outputs": [],
      "source": [
        "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\"stt_en_conformer_ctc_large_ls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and Vocabulary"
      ],
      "metadata": {
        "id": "sXu1xsHuFhYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "gvfc7u4XHhta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJiHEEgWiCUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffbcd86-0772-4ef3-daaa-37481f108264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁he', 'll', 'o', '▁w', 'or', 'l', 'd']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = asr_model.tokenizer.tokenizer\n",
        "print(tokenizer.encode_as_pieces('hello world'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### output units\n",
        "\n",
        "vocab: 1+127+1 (<unk> + tokens + blank)\n"
      ],
      "metadata": {
        "id": "acPDKCJzHdO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IafEbezk-kG",
        "outputId": "658289b9-901e-47bd-9202-9d898df4712e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', 'e', 's', '▁', 't', 'a', 'o', 'i', '▁the', 'd', 'l', 'n', '▁a', 'm', 'y', 'u', '▁s', 'p', 'ed', 'c', '▁and', 're', '▁to', '▁of', 'r', 'w', 'ing', '▁w', 'h', '▁p', '▁c', 'er', 'f', 'k', 'ar', '▁in', '▁f', '▁b', 'g', 'an', 'in', '▁i', 'en', '▁he', 'le', '▁g', 'or', 'll', 'b', '▁be', 'ro', 'st', 'on', '▁d', 'v', 'ly', 'ce', 'ur', 'es', '▁that', '▁o', 'us', '▁was', '▁it', '▁th', 've', 'ch', 'un', 'al', '▁t', '▁ma', 'ri', '▁you', '▁on', 'ver', 'ent', '▁for', '▁re', 'ra', \"'\", '▁his', 'ir', 'ter', '▁with', '▁her', 'it', 'th', '▁mo', '▁me', '▁ha', '▁e', '▁as', 'tion', '▁had', '▁not', '▁no', '▁do', 'ther', '▁but', '▁st', '▁she', '▁is', 'igh', '▁ho', '▁lo', 'ng', '▁him', '▁an', 'ck', 'j', 'ugh', '▁de', '▁li', '▁mi', '▁la', '▁my', '▁con', '▁have', '▁this', '▁which', 'q', '▁up', '▁said', '▁from', '▁who', '▁ex', 'x', 'z']\n"
          ]
        }
      ],
      "source": [
        "vocab = asr_model.tokenizer.vocab\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_KTXv1GlEWQ"
      },
      "outputs": [],
      "source": [
        "with open('tokens.txt', 'w') as f:\n",
        "  for k, v in enumerate(vocab):\n",
        "    f.write(str(v) + ' '+str(k)+ '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### words\n",
        "\n",
        "'word.raw' file: All words in language model(librispeech dataset)\n"
      ],
      "metadata": {
        "id": "A3L45dyuHVav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGvpRO3mmT9r"
      },
      "outputs": [],
      "source": [
        "with open('word.raw', 'r') as f:\n",
        "  wlist = f.read().splitlines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbghkk_yoQY2"
      },
      "outputs": [],
      "source": [
        "wlist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('words.txt', 'w') as f:\n",
        "  for k, v, in enumerate(wlist):\n",
        "    f.write(str(v)+' '+str(k)+'\\n')"
      ],
      "metadata": {
        "id": "gJTsUqhMLdOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lexicon"
      ],
      "metadata": {
        "id": "mkORY0XXHqdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wordpiece"
      ],
      "metadata": {
        "id": "XvKwqLJ0HwU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcusTLMaov59"
      },
      "outputs": [],
      "source": [
        "pieces = []\n",
        "for i in wlist:\n",
        "  pieces.append(tokenizer.encode_as_pieces(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lexicon format\n",
        "\n",
        "lexicon: word token pair (except for special symbols)"
      ],
      "metadata": {
        "id": "RB1FM8ARH00Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sqzAO5co1FN"
      },
      "outputs": [],
      "source": [
        "lexicon = list(zip(wlist, pieces))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7ZHpYYNoRPk"
      },
      "outputs": [],
      "source": [
        "with open('lexicon.txt', \"w\", encoding=\"utf-8\") as f:\n",
        "  for word, tokens in lexicon[1:-1]: # special symbol removal\n",
        "    f.write(f\"{word} {' '.join(tokens)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lexicon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FTbTBLhzOVK",
        "outputId": "ec670744-4920-421a-9640-f31edb112581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "976868"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test set logits"
      ],
      "metadata": {
        "id": "sE6CxBI-H4Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set load\n",
        "\n",
        "Librispeech test-clean set\n",
        "\n",
        "HuggingFace URL: https://huggingface.co/datasets/kresnik/librispeech_asr_test\n",
        "\n"
      ],
      "metadata": {
        "id": "MVQK2s4nJYiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqwOw_6jpA29"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"kresnik/librispeech_asr_test\", \"clean\")\n",
        "test_ds = ds['test']\n",
        "fl = test_ds['file']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## logit extraction"
      ],
      "metadata": {
        "id": "DsMpi9mLJTfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting log-probabilities using Nemo model and huggingface dataset"
      ],
      "metadata": {
        "id": "UNs5zkNjrMwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "40eLxx5OphsW"
      },
      "outputs": [],
      "source": [
        "r = asr_model.transcribe(fl, logprobs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save logits\n",
        "\n",
        "save logits as pt file"
      ],
      "metadata": {
        "id": "rBLohm_6JcsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWLkzX7FrYI4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "out_list = []\n",
        "for i in r:\n",
        "  out_list.append(torch.tensor(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU1JEidCt9Hm"
      },
      "outputs": [],
      "source": [
        "out_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlKsmdz_t-E2"
      },
      "outputs": [],
      "source": [
        "torch.save(out_list, 'logits.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SUPtWZCuDbF",
        "outputId": "8728d9d2-b59c-4a12-b589-156bdb44f09b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([303, 129])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_list[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save reference"
      ],
      "metadata": {
        "id": "DnUPs7EzJfCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "save reference text"
      ],
      "metadata": {
        "id": "bifgdkSKrfP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUcAouI9u0sg"
      },
      "outputs": [],
      "source": [
        "with open('ref.txt', 'w') as f:\n",
        "  for i in test_ds['text']:\n",
        "    f.write(i+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmiWToyOxCbr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nemo_playground.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jsCEydQbFU72",
        "tX-kIZVxhwu-"
      ],
      "authorship_tag": "ABX9TyObvl/PwtP/4rMwL/4vOWug",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}